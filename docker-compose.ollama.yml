version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: doc-mate-ollama
    ports:
      - "11434:11434"
    volumes:
      # Persist model data so you don't re-download 40GB every time
      - ollama-models:/root/.ollama
    restart: unless-stopped

    # Resource limits to prevent system crashes
    deploy:
      resources:
        limits:
          # Limit memory to prevent OOM crashes
          # 10GB for llama3.1:8b (8GB model + 2GB overhead)
          memory: 10G
          cpus: '4.0'  # Limit CPU usage
        reservations:
          memory: 8G  # Minimum memory reservation

    # Optional: GPU support (uncomment if you have NVIDIA GPU)
    # environment:
    #   NVIDIA_VISIBLE_DEVICES: all
    #   OLLAMA_NUM_PARALLEL: 1  # Limit concurrent requests with GPU
    #   OLLAMA_MAX_LOADED_MODELS: 1  # Only keep one model in memory

    # Uncomment for GPU support (requires nvidia-docker)
    # Note: GPU support requires updating the deploy.resources section above
    # deploy:
    #   resources:
    #     limits:
    #       memory: 6G
    #     reservations:
    #       memory: 4G
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  ollama-models:
    name: doc-mate-ollama-models
