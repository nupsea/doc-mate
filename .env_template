# OpenAI API Key (required for OpenAI provider)
OPENAI_API_KEY=your-openai-api-key-here

# LLM Provider Configuration
LLM_PROVIDER=openai  # Options: openai, local
PRIVACY_MODE=false  # Set to true to force local-only (no API calls)
LLM_ENABLE_FALLBACK=true  # Enable fallback to OpenAI if primary fails
LLM_ENABLE_JUDGE=true  # Enable response quality assessment (auto-disabled for local)

# Local Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b  # Default: 8B model (4.7GB download, 8GB RAM, 10GB Docker limit)

# PERFORMANCE OPTIMIZATIONS (automatic for local LLM):
# - Judge disabled (saves 50% request time, avoids JSON parsing issues)
# - max_tokens set to 1536 (balanced for 8B model)
# - Temperature set to 0 (more deterministic function calling)
# - Simplified prompts (better for function calling)
# - Docker memory limit: 10GB (for llama3.2:8b)
# - Concurrent requests limited to 2

# LOCAL LLM CAPABILITIES (llama3.1:8b):
# Works well for: questions, summaries, multi-doc comparisons, function calling
# Good balance of speed and capability
# TIP: For fastest responses or most complex queries, use LLM_PROVIDER=openai

# PostgreSQL credentials
PG_USER=your-postgres-username
PG_PASS=your-postgres-password